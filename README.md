# ğŸš€ TP Hadoop â€” WordCount avec Docker (Nouvelle Version 2025)

Setup 100% automatique dâ€™un cluster **Hadoop 3.x** + compilation + exÃ©cution dâ€™un **WordCount Java**.
Tu peux tout lancer en *AUTO* ou gÃ©rer toi-mÃªme en *MANUEL*.

Structure du projet :

```
hadoop/
â”œâ”€â”€ install.sh
â”œâ”€â”€ compile_and_run.sh
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ docker-compose.dev.yml
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ wordcount/
â”‚   â”‚   â”œâ”€â”€ WCDriver.java
â”‚   â”‚   â”œâ”€â”€ WCMapper.java
â”‚   â”‚   â””â”€â”€ WCReducer.java
â”‚   â””â”€â”€ wordcountenseignant/
â”‚   â”‚   â”œâ”€â”€ WCDriver.java
â”‚   â”‚   â”œâ”€â”€ WCMapper.java
â”‚   â”‚   â””â”€â”€ WCReducer.java
â””â”€â”€ data/
    â””â”€â”€ file.txt
```

---

# âš¡ï¸ INSTALLATION AUTOMATIQUE (RECOMMANDÃ‰E)

## 1) Installation complÃ¨te Hadoop + Dev + Java

```bash
chmod +x install.sh
./install.sh
```

Lance automatiquement :
- le cluster Hadoop
- le conteneur Dev
- l'installation de Java 11

---

## 2) Compilation + JAR + HDFS + WordCount

```bash
chmod +x compile_and_run.sh
./compile_and_run.sh
```

Ce script :
- compile le WordCount
- gÃ©nÃ¨re le `.jar`
- upload le fichier dans HDFS
- exÃ©cute le job
- affiche le rÃ©sultat

---

# ğŸ¯ Options avancÃ©es

Exemple pour changer la classe main + fichier :

```bash
./compile_and_run.sh -mainClass=src.wordcountenseignant.WCDriver ./data/file.txt
```

### ParamÃ¨tres disponibles

| Option | Description | DÃ©faut |
|--------|-------------|--------|
| `-container=<nom>` | Conteneur Hadoop | namenode |
| `-jarName=<nom>` | Nom du JAR gÃ©nÃ©rÃ© | wc.jar |
| `-mainClass=<classe>` | Classe Java principale | src.wordcountenseignant.WCDriver |
| `input_file` | Fichier local Ã  envoyer dans HDFS | `./src/wordcount/file.txt` |

Afficher lâ€™aide :

```bash
./compile_and_run.sh --help
```

---

# ğŸ§± INSTALLATION MANUELLE

## 1) Lancer Hadoop

```bash
docker compose up -d
```

## 2) Lancer Dev

```bash
docker compose -f docker-compose.dev.yml up -d
docker exec -it hadoop-dev bash
```

## 3) Installer Java

```bash
apt update
apt install -y openjdk-11-jdk
javac -version
```

## 4) Copier le code Java

```bash
docker cp ./src namenode:/root/
docker cp ./data/file.txt namenode:/root/
```

## 5) Compiler

```bash
find /root/src -name '*.java' > sources.txt
javac -cp "$(hadoop classpath)" -d build @sources.txt
```

## 6) JAR

```bash
jar -cvf wc.jar -C build .
```

## 7) HDFS

```bash
hdfs dfs -mkdir -p /data
hdfs dfs -put -f /root/file.txt /data
```

## 8) ExÃ©cution

```bash
hadoop jar wc.jar src.wordcount.WCDriver /data /output
```

## 9) Lire rÃ©sultat

```bash
hdfs dfs -cat /output/part-r-00000
```

---

# ğŸ‰ RÃ©sultat

Tu as :
- un cluster Hadoop fonctionnel
- un MapReduce Java compilÃ©
- un `.jar` exÃ©cutable
- un fichier injectÃ© dans HDFS
- un WordCount qui tourne

Parfait pour valider ton TP et passer Ã  Spark ğŸ”¥ğŸ˜

